version: 2.1

orbs:
  
  slack: circleci/slack@4.10.1

commands:

  failure_notifcation:
    steps:
      - slack/notify:
          event: fail
          channel: udapeople-alerting
          template: basic_fail_1

  install_awscli:
    steps:
      - run:
          command: |
            curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
            unzip awscliv2.zip
            sudo ./aws/install

  install_ansible:
    steps:
      - run:
          command: |
            sudo apt update
            sudo apt install software-properties-common
            sudo add-apt-repository --yes --update ppa:ansible/ansible
            sudo apt install ansible

  install_nodejs:
    description: Install Node.js 
    steps:
      - run:
          name: Install Node.js 
          command: |
            curl -fsSL https://deb.nodesource.com/setup_14.x | sudo -E bash -
            sudo apt install -y nodejs

  destroy-environment:
    description: Destroy back-end and front-end cloudformation stacks given a workflow ID.
    parameters:
      workflow_ID:
        type: string
        default: ${CIRCLE_WORKFLOW_ID:0:7}   
    steps:
      - run:
          name: Destroy environments
          when: on_fail
          command: |
             aws cloudformation delete-stack --stack-name udapeople-backend-<< parameters.workflow_ID >>
             aws s3 rm s3://udapeople-<<parameters.workflow_ID>> --recursive
             aws cloudformation delete-stack --stack-name udapeople-frontend-<< parameters.workflow_ID >>

  revert-migrations:
    description: Revert the last migration if successfully run in the current workflow.
    parameters:
      workflow_ID:
        type: string
        default: ${CIRCLE_WORKFLOW_ID:0:7}  
    steps:
      - run:
          name: Revert migrations
          when: on_fail 
          command: |
          
            SUCCESS=$(curl --insecure https://kvdb.io/${KVDB_BUCKET}/migration_<< parameters.workflow_ID >>)
            if(( $SUCCESS==1 )); 
            then
              cd ~/project/backend
              npm install
              npm run migration:revert
            fi
            
jobs:
  build-frontend:
    docker:
      - image: cimg/node:12.13.1
    steps:
      - checkout
      - restore_cache:
          keys: [frontend-dependencies]
      - run:
          name: Build front-end
          command: |
            cd frontend
            npm install
            # npm update
            # npm audit fix 
            # npm audit fix --force
            npm run build
            # npm audit fix --force


      - save_cache:
          paths: [frontend/node_modules]
          key: frontend-dependencies
      - failure_notifcation

  build-backend:
    docker:
      - image: cimg/node:12.13.1
    steps:
      - checkout
      - restore_cache:
          keys: [backend-dependencies]
      - run:
          name: Back-end build
          command: |
            cd backend
            npm install
            npm run build
      - save_cache:
          paths: [backend/node_modules]
          key: backend-dependencies
      - failure_notifcation

  test-frontend:
    docker:
      - image: cimg/node:12.13.1
    steps:
      - checkout
      - restore_cache:
          keys: [frontend-dependencies]
      - run:
          name: Front-end Unit Test
          command: |
            cd frontend
            npm install
            npm test
      - failure_notifcation

  test-backend:
    docker:
      - image: cimg/node:12.13.1
    steps:
      - checkout
      - restore_cache:
          keys: [backend-dependencies]
      - run:
          name: Back-end Unit Test
          command: |
            cd backend
            npm install
            npm test
      - failure_notifcation

  scan-frontend:
    docker:
      - image: cimg/node:12.13.1
    steps:
      - checkout
      - restore_cache:
          keys: [frontend-dependencies]
      - run:
          name: Front-end scan
          command: |
            cd frontend
            npm install
            npm audit fix --force --audit-level=critical
            npm audit fix --force --audit-level=critical

            npm audit --audit-level=critical
      - failure_notifcation


  scan-backend:
    docker:
      - image: cimg/node:12.13.1
    steps:
      - checkout
      - restore_cache:
          keys: [backend-dependencies]
      - run:
          name: Back-end scan
          command: |
             cd backend
             npm install
             npm audit fix --force --audit-level=critical
             npm audit fix --force --audit-level=critical
             npm audit --audit-level=critical  
      - failure_notifcation

  deploy-infrastructure:
    docker:
      - image: cimg/base:stable
    steps:
      - checkout
      - install_awscli
      - run:
          name: Ensure back-end infrastructure exists
          command: |
            aws cloudformation deploy \
              --template-file .circleci/files/backend.yml \
              --tags project=udapeople \
              --stack-name "udapeople-backend-${CIRCLE_WORKFLOW_ID:0:7}" \
              --parameter-overrides ID="${CIRCLE_WORKFLOW_ID:0:7}"  
              
      - run:
          name: Ensure front-end infrastructure exist
          command: |
            aws cloudformation deploy \
              --template-file .circleci/files/frontend.yml \
              --tags project=udapeople \
              --stack-name "udapeople-frontend-${CIRCLE_WORKFLOW_ID:0:7}" \
              --parameter-overrides ID="${CIRCLE_WORKFLOW_ID:0:7}"  
              
      - run:
          name: Add back-end ip to ansible inventory
          command: |
            BACKEND_PUBLIC_IP=$(aws ec2 describe-instances \
            --filters "Name=tag:Name,Values=backend-${CIRCLE_WORKFLOW_ID:0:7}" \
            --query 'Reservations[*].Instances[*].PublicIpAddress' \
            --output text)
            echo $BACKEND_PUBLIC_IP >> .circleci/ansible/inventory.txt
            cat .circleci/ansible/inventory.txt
            
      - persist_to_workspace: 
          root: ~/
          paths:
            - project/.circleci/ansible/inventory.txt

      - destroy-environment
    

  configure-infrastructure:
    docker:
      - image: cimg/base:stable
    steps:
      - checkout
      - install_awscli
      - install_ansible
      - add_ssh_keys:
          fingerprints: ["34:0d:60:21:db:cf:e7:76:e1:68:06:f5:61:4a:30:ed"]
      
      - attach_workspace:
          at: ~/

      - run:
          name: Configure server
          command: |
            cd .circleci/ansible
            cat inventory.txt
            ansible-playbook -i inventory.txt configure-server.yml
      - destroy-environment 

  # run-migrations:
  #   docker:
  #     - image: node
  #   steps:
  #     - checkout   
  #     - run:
  #         name: Run migrations
  #         command: |
  #           cd backend
  #           npm install
  #           npm run migrations > migrations_dump.txt
  #     - run:
  #         name: Send migration results to kvdb.io
  #         command: |
  #           cat ~/project/backend/migrations_dump.txt
  #           if grep -q "has been executed successfully." ~/project/backend/migrations_dump.txt
  #           then
  #               curl https://kvdb.io/${KVDB_BUCKET}/migration_${CIRCLE_WORKFLOW_ID:0:7}  -d '1'
  #           fi

  #     - destroy-environment  
  #     - revert-migrations

  run-migrations:
    docker:
      - image: cimg/node:13.8.0
    steps:
      - checkout
      - install_awscli
      - run:
          name: Run migrations
          command: |
            cd backend
            npm install
            npm run migrations > migrations_dump.txt
      - run:
          name: Send migration status to kvdb.io
          command: |
            cat ~/project/backend/migrations_dump.txt
            if grep -q "has been executed successfully." ~/project/backend/migrations_dump.txt
            then
              curl https://kvdb.io/${KVDB_BUCKET}/migration_${CIRCLE_WORKFLOW_ID:0:7}  -d '1'
            fi
      - destroy-environment
      - revert-migrations

  deploy-frontend:
    docker:
      - image: cimg/node:12.13.1
    steps:
      - checkout
      - install_awscli
      - restore_cache:
          keys: [frontend-dependencies]
      - run:
          name: Install dependencies
          command: |
            cd frontend
            npm install
      - run:
          name: Get backend url
          command: |
            BACKEND_PUBLIC_IP=$(aws ec2 describe-instances \
            --filters "Name=tag:Name,Values=backend-${CIRCLE_WORKFLOW_ID:0:7}" \
            --query 'Reservations[*].Instances[*].PublicIpAddress' \
            --output text)

            echo "API_URL=http://${BACKEND_PUBLIC_IP}:3030" >> frontend/.env
            cat frontend/.env
      - run:
          name: Deploy frontend objects
          command: |
            cd frontend
            npm run build
            aws s3 cp dist s3://udapeople-${CIRCLE_WORKFLOW_ID:0:7}  --recursive

      - destroy-environment    
      - revert-migrations

  deploy-backend:
    docker:
      - image: cimg/base:stable
    steps:
      - checkout
      - install_awscli
      - install_ansible
      - install_nodejs
      - add_ssh_keys:
          fingerprints: ["34:0d:60:21:db:cf:e7:76:e1:68:06:f5:61:4a:30:ed"]
      
      - attach_workspace:
          at: ~/
      
      - restore_cache:
          keys: [backend-dependencies]
      
      - run:
          name: Install dependencies
          command: |
            cd backend
            npm install 

      - run:
          name: package backend
          command: |
            cd backend
            npm run build
            # Zip the directory
            tar -czf artifact.tar.gz dist/* package*

            cd ..
            cp backend/artifact.tar.gz .circleci/ansible/roles/deploy/files

      - run:
          name: deploy backend
          command: |
            export TYPEORM_MIGRATIONS_DIR=./migrations
            export TYPEORM_ENTITIES=./modules/domain/**/*.entity{.ts,.js}
            export TYPEORM_MIGRATIONS=./migrations/*.ts

            cd .circleci/ansible
            echo "Contents  of the inventory.txt file is -------"
            cat inventory.txt
            ansible-playbook -i inventory.txt deploy-backend.yml
      # Here's where you will add some code to rollback on failure  
      - destroy-environment    
      - revert-migrations

  # deploy-infrastructure:
  #   docker:
  #     - image: cimg/base:stable
  #   steps:
  #     - checkout
  #     - install_awscli
  #     - run:
  #         name: Ensure back-end infrastructure exists
  #         command: |
  #           aws cloudformation deploy \
  #             --template-file .circleci/files/backend.yml \
  #             --tags project=udapeople \
  #             --stack-name "udapeople-backend-${CIRCLE_WORKFLOW_ID:0:7}" \
  #             --parameter-overrides ID="${CIRCLE_WORKFLOW_ID:0:7}"
  #     - run:
  #         name: Ensure front-end infrastructure exist
  #         command: |
  #           aws cloudformation deploy \
  #             --template-file .circleci/files/frontend.yml \
  #             --tags project=udapeople \
  #             --stack-name "udapeople-frontend-${CIRCLE_WORKFLOW_ID:0:7}" \
  #             --parameter-overrides ID="${CIRCLE_WORKFLOW_ID:0:7}"
  #     - run:
  #         name: Add back-end ip to ansible inventory
  #         command: |
  #           BACKEND_PUBLIC_IP=$(aws ec2 describe-instances \
  #             --filters "Name=tag:Name,Values=backend-${CIRCLE_WORKFLOW_ID:0:7}" \
  #             --query 'Reservations[*].Instances[*].PublicIpAddress' \
  #             --output text)
  #           echo $BACKEND_PUBLIC_IP >> .circleci/ansible/inventory.txt
  #           cat .circleci/ansible/inventory.txt
  #     - persist_to_workspace:
  #         root: ~/
  #         paths:
  #           - project/.circleci/ansible/inventory.txt
      
  #     - destroy-environment

  # configure-infrastructure:
  #   docker:
  #     - image: cimg/base:stable
  #   steps:
  #     - checkout
  #     - install_awscli
  #     - install_ansible
  #     - add_ssh_keys:
  #         fingerprints: ["d9:06:93:7c:dd:ac:25:bf:3d:f3:10:f5:5d:69:f6:67"]
  #     - attach_workspace:
  #         at: ~/
  #     - run:
  #         name: Configure Server
  #         command: |
  #           cd .circleci/ansible
  #           cat inventory.txt
  #           ansible-playbook -i inventory.txt configure-server.yml
  #     - destroy-environment
  # run-migrations:
  #   docker:
  #     - image: cimg/node:12.13.1
  #   steps:
  #     - checkout
  #     - install_awscli
  #     - run:
  #         name: Run migrations
  #         command: |
  #           cd backend
  #           npm install
  #           npm run migrations > migrations_dump.txt
  #     - run:
  #         name: Send migration status to kvdb.io
  #         command: |
  #           if grep -q "has been executed successfully." ~/project/backend/migrations_dump.txt
  #           then
  #             c  https://kvdb.io/${KVDB_BUCKET}/migration_${CIRCLE_WORKFLOW_ID:0:7}  -d '1'
  #           fi
  #     - destroy-environment
  #     - revert-migrations
  # deploy-frontend:
  #   docker:
  #     - image: cimg/base:stable
  #   steps:
  #     - checkout
  #     - install_awscli
  #     - install_nodejs
  #     - restore_cache:
  #         keys: [frontend-deps]
  #     - run:
  #         name: Install dependencies
  #         command: |
  #           cd frontend
  #           npm install
  #     - run:
  #         name: Get backend url
  #         command: |
  #           BACKEND_PUBLIC_IP=$(aws ec2 describe-instances \
  #             --filters "Name=tag:Name,Values=backend-${CIRCLE_WORKFLOW_ID:0:7}" \
  #             --query 'Reservations[*].Instances[*].PublicIpAddress' \
  #             --output text)
  #           echo "API_URL=http://${BACKEND_PUBLIC_IP}:3030" >> frontend/.env
  #           cat frontend/.env
  #     - run:
  #         name: Deploy frontend objects
  #         command: |
  #           cd frontend
  #           npm run build
  #           aws s3 cp dist s3://udapeople-${CIRCLE_WORKFLOW_ID:0:7} --recursive
  #     - destroy-environment
  #     - revert-migrations
  # deploy-backend:
  #   docker:
  #     - image: cimg/base:stable
  #   steps:
  #     - checkout
  #     - install_awscli
  #     - install_ansible
  #     - install_nodejs
  #     - add_ssh_keys:
  #         fingerprints: ["d9:06:93:7c:dd:ac:25:bf:3d:f3:10:f5:5d:69:f6:67"]
  #     - attach_workspace:
  #         at: ~/
  #     - restore_cache:
  #         keys: [backend-deps]
  #     - run:
  #         name: Install dependencies
  #         command: |
  #           cd backend
  #           npm install
  #     - run:
  #         name: Package Backend
  #         command: |
  #           cd backend
  #           npm run build
  #           tar -czf artifact.tar.gz dist/* package*
  #           cd ..
  #           cp backend/artifact.tar.gz .circleci/ansible/roles/deploy/files
  #     - run:
  #         name: Deploy backend
  #         command: |
  #           export TYPEORM_MIGRATIONS_DIR=./migrations
  #           export TYPEORM_ENTITIES=./modules/domain/**/*.entity{.ts,.js}
  #           export TYPEORM_MIGRATIONS=./migrations/*.ts
  #           cd .circleci/ansible
  #           cat inventory.txt
  #           ansible-playbook -i inventory.txt deploy-backend.yml
  #     - destroy-environment
  #     - revert-migrations

  smoke-test:
    docker:
      - image: cimg/node:12.13.1
    steps:
      - checkout
      - install_awscli

      - run:
          name: Backend smoke test.
          command: |
            BACKEND_PUBLIC_IP=$(aws ec2 describe-instances \
            --filters "Name=tag:Name,Values=backend-${CIRCLE_WORKFLOW_ID:0:7}" \
            --query 'Reservations[*].Instances[*].PublicIpAddress' \
            --output text)

            export API_URL=http://${BACKEND_PUBLIC_IP}:3030
            echo ${API_URL}
            
            if curl -s $API_URL/api/status | grep "ok"
            then
              exit 0
            else
              exit 1
            fi 
            
      - run:
          name: Frontend smoke test.
          command: |
            FRONTEND_WEBSITE=http://udapeople-${CIRCLE_WORKFLOW_ID:0:7}.s3-website.${AWS_DEFAULT_REGION}.amazonaws.com
            if curl -s $FRONTEND_WEBSITE | grep "Welcome"
            then
              exit 0 
            else 
              exit 1
            fi 

      - destroy-environment    
      - revert-migrations 

  cloudfront-update:
    docker:
      - image: cimg/node:12.13.1
    steps:
      - checkout
      - install_awscli
      
      - run:
          name: Save Old Workflow ID to kvdb.io
          command: |
            export OLD_WORKFLOW_ID=$(aws cloudformation \
            list-exports -- query "Exports[?Name==\'WorkflowID\'].Value" \
            --no-paginate --output text)

            echo "Old Workflow ID:"$OLD_WORKFLOW_ID"
            curl https://kvdb.io/${KVDB_BUCKET}/old_workflow_id -d "${OLD_WORKFLOW_ID}""
            
      - run:
          name: Update cloudfront distribution
          command: |
              aws cloudformation deploy \
              --template-file .circleci/files/frontend.yml \
              --stack-name "initialStack" \
              --parameter-overrides ID="${CIRCLE_WORKFLOW_ID:0:7}" 

      - destroy-environment    
      - revert-migrations 

  cleanup:
      docker:
        - image: cimg/node:12.13.1
      steps:
        - checkout
        - install_awscli

        - run:
            name: Remove old stacks and files
            command: |
              export STACKS=($(aws cloudformation list-stacks 
              --query "StackSummaries[*].StackName" \
              --stack-status-filter CREATE_COMPLETE --no-paginate --output text)) 
              
              echo Stack names: "${STACKS[@]}" 

              export OldWorkflowID=$(curl --insecure https://kvdb.io/${KVDB_BUCKET}/old_workflow_id)
              echo Old Workflow ID: $OldWorkflowID
              
              if [[ "${STACKS[@]}" =~ "${OldWorkflowID}" ]]
              then
                aws s3 rm "s3://udapeople-${OldWorkflowID}" --recursive
                aws cloudformation delete-stack --stack-name "udapeople-backend-${OldWorkflowID}"
                aws cloudformation delete-stack --stack-name "udapeople-frontend-${OldWorkflowID}"
              fi
              
  succss_notification:
    docker:
      - image: cimg/base:stable        
    steps:
      - slack/notify:
          event: pass
          channel: udapeople-alerting
          template: success_tagged_deploy_1

workflows:
  Give Your Application Auto-deploy Superpowers:
    jobs:
      - build-frontend
      - build-backend
      - test-frontend:
          requires: [build-frontend]
      - test-backend:
          requires: [build-backend]
      - scan-backend:
          requires: [build-backend]
      - scan-frontend:
          requires: [build-frontend]
      - deploy-infrastructure:
          requires: [test-frontend, test-backend, scan-frontend, scan-backend]
          filters:
            branches:
              only: [master]
      - configure-infrastructure:
          requires: [deploy-infrastructure]
      - run-migrations:
          requires: [configure-infrastructure]
      # - deploy-frontend:
      #     requires: [run-migrations]
      # - deploy-backend:
      #     requires: [run-migrations]
      # - smoke-test:
      #     requires: [deploy-backend, deploy-frontend]
      # - cloudfront-update:
      #     requires: [smoke-test]
      # - cleanup:
      #     requires: [cloudfront-update]
      # - succss_notification:
      #     requires: [cleanup]


# master